{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A From Scratch 3 Layer Neural Network (NN) Example\n",
    "\n",
    "This is a simple example of building a neural net from scratch using python without using a deep learning library like tensorflow. I will try to spare folks the math while attempting to demystify NN in plain english as much as possible. The solution assumes that you have some idea of what a neural net is. If you do not what it is or would like a refresher, check out the first of the 4 awesome videos on youtube: https://www.youtube.com/watch?v=aircAruvnKk.\n",
    "\n",
    "NN can also be referred to as Artificial Neural Network (ANN) or Multi Level Perceptron (MLP) to differentiate it from other networks like Convolutional Neural Networks (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "A farmer in Italy was having a problem with his labelling machine: it mixed up the labels of three wine cultivars. Now he has 178 bottles left, and nobody knows which cultivar made them! To help this poor man, we will build a classifier that recognizes the wine based on 13 attributes of the wine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.freecodecamp.org/building-a-3-layer-neural-network-from-scratch-99239c4af5d3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before approaching our solution from a neural net's perspective, let us read & get some basic understanding of the data first. For simplicity, we will be reading from a local copy, however the data is also available hrough the below link and also has data set definitions.\n",
    "\n",
    "[Wine dataset](https://archive.ics.uci.edu/ml/datasets/Wine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing all the required libraries\n",
    "\n",
    "# modules for data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# visualization modules\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# machine learning library and modules\n",
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining path to the data and the data source itself\n",
    "rel_data_path=\"../../data/\"\n",
    "filename=\"Wine.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Malic acid</th>\n",
       "      <th>Ash</th>\n",
       "      <th>Alcalinity of ash</th>\n",
       "      <th>Magnesium</th>\n",
       "      <th>Total phenols</th>\n",
       "      <th>Flavanoids</th>\n",
       "      <th>Nonflavanoid phenols</th>\n",
       "      <th>Proanthocyanins</th>\n",
       "      <th>Color intensity</th>\n",
       "      <th>Hue</th>\n",
       "      <th>OD280/OD315 of diluted wines</th>\n",
       "      <th>Proline</th>\n",
       "      <th>Cultivar 1</th>\n",
       "      <th>Cultivar 2</th>\n",
       "      <th>Cultivar 3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.518613</td>\n",
       "      <td>-0.562250</td>\n",
       "      <td>0.232053</td>\n",
       "      <td>-1.169593</td>\n",
       "      <td>1.913905</td>\n",
       "      <td>0.808997</td>\n",
       "      <td>1.034819</td>\n",
       "      <td>-0.659563</td>\n",
       "      <td>1.224884</td>\n",
       "      <td>0.251717</td>\n",
       "      <td>0.362177</td>\n",
       "      <td>1.847920</td>\n",
       "      <td>1.013009</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.246290</td>\n",
       "      <td>-0.499413</td>\n",
       "      <td>-0.827996</td>\n",
       "      <td>-2.490847</td>\n",
       "      <td>0.018145</td>\n",
       "      <td>0.568648</td>\n",
       "      <td>0.733629</td>\n",
       "      <td>-0.820719</td>\n",
       "      <td>-0.544721</td>\n",
       "      <td>-0.293321</td>\n",
       "      <td>0.406051</td>\n",
       "      <td>1.113449</td>\n",
       "      <td>0.965242</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.196879</td>\n",
       "      <td>0.021231</td>\n",
       "      <td>1.109334</td>\n",
       "      <td>-0.268738</td>\n",
       "      <td>0.088358</td>\n",
       "      <td>0.808997</td>\n",
       "      <td>1.215533</td>\n",
       "      <td>-0.498407</td>\n",
       "      <td>2.135968</td>\n",
       "      <td>0.269020</td>\n",
       "      <td>0.318304</td>\n",
       "      <td>0.788587</td>\n",
       "      <td>1.395148</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.691550</td>\n",
       "      <td>-0.346811</td>\n",
       "      <td>0.487926</td>\n",
       "      <td>-0.809251</td>\n",
       "      <td>0.930918</td>\n",
       "      <td>2.491446</td>\n",
       "      <td>1.466525</td>\n",
       "      <td>-0.981875</td>\n",
       "      <td>1.032155</td>\n",
       "      <td>1.186068</td>\n",
       "      <td>-0.427544</td>\n",
       "      <td>1.184071</td>\n",
       "      <td>2.334574</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.295700</td>\n",
       "      <td>0.227694</td>\n",
       "      <td>1.840403</td>\n",
       "      <td>0.451946</td>\n",
       "      <td>1.281985</td>\n",
       "      <td>0.808997</td>\n",
       "      <td>0.663351</td>\n",
       "      <td>0.226796</td>\n",
       "      <td>0.401404</td>\n",
       "      <td>-0.319276</td>\n",
       "      <td>0.362177</td>\n",
       "      <td>0.449601</td>\n",
       "      <td>-0.037874</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Alcohol  Malic acid       Ash  Alcalinity of ash  Magnesium  \\\n",
       "0  1.518613   -0.562250  0.232053          -1.169593   1.913905   \n",
       "1  0.246290   -0.499413 -0.827996          -2.490847   0.018145   \n",
       "2  0.196879    0.021231  1.109334          -0.268738   0.088358   \n",
       "3  1.691550   -0.346811  0.487926          -0.809251   0.930918   \n",
       "4  0.295700    0.227694  1.840403           0.451946   1.281985   \n",
       "\n",
       "   Total phenols  Flavanoids  Nonflavanoid phenols  Proanthocyanins  \\\n",
       "0       0.808997    1.034819             -0.659563         1.224884   \n",
       "1       0.568648    0.733629             -0.820719        -0.544721   \n",
       "2       0.808997    1.215533             -0.498407         2.135968   \n",
       "3       2.491446    1.466525             -0.981875         1.032155   \n",
       "4       0.808997    0.663351              0.226796         0.401404   \n",
       "\n",
       "   Color intensity       Hue  OD280/OD315 of diluted wines   Proline  \\\n",
       "0         0.251717  0.362177                      1.847920  1.013009   \n",
       "1        -0.293321  0.406051                      1.113449  0.965242   \n",
       "2         0.269020  0.318304                      0.788587  1.395148   \n",
       "3         1.186068 -0.427544                      1.184071  2.334574   \n",
       "4        -0.319276  0.362177                      0.449601 -0.037874   \n",
       "\n",
       "   Cultivar 1  Cultivar 2  Cultivar 3  \n",
       "0           1           0           0  \n",
       "1           1           0           0  \n",
       "2           1           0           0  \n",
       "3           1           0           0  \n",
       "4           1           0           0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = rel_data_path+filename\n",
    "df = pd.read_csv(file_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Alcohol', 'Malic acid', 'Ash', 'Alcalinity of ash', 'Magnesium',\n",
       "       'Total phenols', 'Flavanoids', 'Nonflavanoid phenols',\n",
       "       'Proanthocyanins', 'Color intensity', 'Hue',\n",
       "       'OD280/OD315 of diluted wines', 'Proline', 'Cultivar 1', 'Cultivar 2',\n",
       "       'Cultivar 3'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the sample data and the column names, we know that the farmer is looking to determine the value for one of the cultivar and wants it to be as accurate as possible based on the other attributes. Our dataset gives us a value of the output that we can use to determine how good the prediction is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a NN\n",
    "\n",
    "#### Defining X & y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get labels\n",
    "y = df[['Cultivar 1', 'Cultivar 2', 'Cultivar 3']].values\n",
    "\n",
    "# Get inputs; \n",
    "X = df.drop(['Cultivar 1', 'Cultivar 2', 'Cultivar 3'], axis = 1)\n",
    "# print(X.shape, y.shape) # Print shapes just to check\n",
    "X = X.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identifying all needed data and values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `X` here consists of 178 rows with 13 columns or 13 _neurons_. This is the input layer of the NN. The `y` is the output and consists of 3 neurons. The goal is to match each input row with one of the output columns using the 13 input columns.\n",
    "\n",
    "The output is considered to be one of the layers and the input typically is not. Hence, for a 3 layer NN, we still need two more layers. This is really an arbitrary number and I am using 3 layer NN as an example. Each of these layers will also have an arbitrary number of neurons which will be defined as we progress. \n",
    "\n",
    "Here is what we need:\n",
    "\n",
    "* An input layer, `X` (already defined) with `n0` neurons\n",
    "* 2 hidden layers `A1` & `A2` with arbitrary number of neurons `n1` & `n2` respectively\n",
    "* A predicted output layer, `y_hat` with `n3` neurons\n",
    "* A choice of activation function `σ1` & `σ2`  for each hidden layer\n",
    "* A set of weights `W1`, `W2` & `W3`, and biases `b1`, `b2`, `b3` between each layer\n",
    "* An `epoch` or number of iterations before a final output is determined\n",
    "* A learning rate `lr` for the model. The learning rate determines how rapidly can the parameters change. If the learning rate is very low, training takes longer. However, if the learning rate is too high, there is a chance to miss the minimum. Choosing a learning rate may involve some trial and error and is beyond the scope of this notebook.\n",
    "\n",
    "A NN process will follow the following steps and use the variables defined above to generate the desired output.\n",
    "1. Take input\n",
    "2. Mutiply 1 by a set of weights and add the biases\n",
    "3. Apply an activation function to 2\n",
    "4. Repeat steps 2 & 3 for each layer\n",
    "5. Return an output\n",
    "6. Calculate error\n",
    "7. Alter the weights based on the error\n",
    "8. Go back to step 2 and repeat for a predefined number of times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining all needed data and values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.02 # learning rate\n",
    "epoch = 4000 # again an arbitrary number for number of iterations\n",
    "n0 = 13 # neurons in input layer \n",
    "n1 = 8 # neurons in hidden layer 1\n",
    "n2 = 5 # neurons in hidden layer 2\n",
    "n3 = 3 # neurons in output layer 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we do not have any values of either the weights `Wi` or the bias `bi`, we will initialize these values using a random number. There are other ways too and the below reads are a great overview or various ways to initialize weights as well as the use of activation functions. \n",
    "\n",
    "_Note:_  \n",
    "[Jun Lu's Blog](http://www.junlulocky.com/actfuncoverview)  \n",
    "[Sagar's Medium Post](https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6)\n",
    "\n",
    "Since we are going to use a type of logistic activation called `softmax` for this example, the initial weights will be randomaly assigned between 0 & 1. We will also start with the biases of 0. ***Why Softmax?:*** The output classifiers are multivariate and mutually exclusive in our example. In other words, the wine shall be classified into one and ONLY one class without a overlap.\n",
    "\n",
    "Wrapping all these weights and biases in a function will allow us to modularize the code and make it easy to trouble shoot later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0) # to create a reproducible example\n",
    "\n",
    "def init_wts_bias(inputs,lyr1,lyr2,outputs):\n",
    "    \n",
    "    # First layer weights & bias\n",
    "    W1 = np.random.random((inputs, lyr1))\n",
    "    b1 = np.zeros((1, lyr1))\n",
    "    \n",
    "    # Second layer weights & bias\n",
    "    W2 = np.random.random((lyr1,lyr2))\n",
    "    b2 = np.zeros((1, lyr2))\n",
    "    \n",
    "    # Third layer weights & bias\n",
    "#     W3 = 2 * np.random.rand(lyr2, outputs) - 1\n",
    "    W3 = np.random.random((lyr2, outputs))\n",
    "    b3 = np.zeros((1,outputs))\n",
    "    \n",
    "    # return all weights and biases\n",
    "    model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2,'W3':W3,'b3':b3}\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = init_wts_bias(13,8,5,3)\n",
    "# temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we define all our functions\n",
    "\n",
    "# Since this is a multi-class classification problem (we have 3 output labels), \n",
    "    # we will use the softmax function for the output layer — A3 — \n",
    "    # because this will compute the probabilities for the classes by spitting out a value between 0 and 1.\n",
    "def softmax(z):\n",
    "    #Calculate exponent term first\n",
    "    exp_scores = np.exp(X - np.max(X))\n",
    "    return exp_scores / np.sum(exp_scores)\n",
    "\n",
    "def cross_entropy(X,y):\n",
    "    \"\"\"\n",
    "    X is the output from fully connected layer (num_examples x num_classes)\n",
    "    y is labels (num_examples x 1)\n",
    "    \tNote that y is not one-hot encoded vector. \n",
    "    \tIt can be computed as y.argmax(axis=1) from one-hot encoded vectors of labels if required.\n",
    "    \"\"\"\n",
    "    m = y.shape[0]\n",
    "    p = softmax(X)\n",
    "    # We use multidimensional array indexing to extract \n",
    "    # softmax probability of the correct label for each sample.\n",
    "    # Refer to https://docs.scipy.org/doc/numpy/user/basics.indexing.html#indexing-multi-dimensional-arrays for understanding multidimensional array indexing.\n",
    "    log_likelihood = -np.log(p[range(m),y])\n",
    "    loss = np.sum(log_likelihood) / m\n",
    "    return loss\n",
    "\n",
    "def softmax_loss(y,y_hat):\n",
    "    # Clipping value\n",
    "    minval = 0.000000000001\n",
    "    # Number of samples\n",
    "    m = y.shape[0]\n",
    "    # Loss formula, note that np.sum sums up the entire matrix and therefore does the job of two sums from the formula\n",
    "    loss = -1/m * np.sum(y * np.log(y_hat.clip(min=minval)))\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def loss_derivative(y_true,y_hat):\n",
    "    return (y_hat-y_true)\n",
    "\n",
    "\n",
    "# hyperbolic tangent (tanh) activation function. Can be otherfunctions like sigmoid.\n",
    "# For this model, we chose to use the tanh activation function for our two hidden layers — A1 and A2 — \n",
    "    # which gives us an output value between 0 and -1.\n",
    "def tanh_derivative(x):\n",
    "    grad = 1 - np.power(x, 2)\n",
    "    return grad\n",
    "\n",
    "def softmax_grad(s): \n",
    "    # Take the derivative of softmax element w.r.t the each logit which is usually Wi * X\n",
    "    # input s is softmax value of the original input x. \n",
    "    # s.shape = (1, n) \n",
    "    # i.e. s = np.array([0.3, 0.7]), x = np.array([0, 1])\n",
    "    # initialize the 2-D jacobian matrix.\n",
    "    s = s.reshape(-1,1)\n",
    "    grad = np.diag(s) - np.dot(s, s.T)\n",
    "    print(grad)\n",
    "    return grad\n",
    "\n",
    "# This is the forward propagation function\n",
    "def forward_prop(model,a0):\n",
    "    \n",
    "    #Start Forward Propagation\n",
    "    \n",
    "    # Load parameters from model\n",
    "    W1, b1, W2, b2, W3, b3 = model['W1'], model['b1'], model['W2'], model['b2'], model['W3'],model['b3']\n",
    "    \n",
    "    # Do the first Linear step \n",
    "    # Z1 is the input layer x times the dot product of the weights + our bias b\n",
    "    z1 = a0.dot(W1) + b1\n",
    "    \n",
    "    # Put it through the first activation function. Also called as transfer function.\n",
    "#     a1 = np.tanh(z1)\n",
    "    a1 = softmax(z1)\n",
    "    \n",
    "    # Second linear step\n",
    "    z2 = a1.dot(W2) + b2\n",
    "    \n",
    "    # Second activation function\n",
    "#     a2 = np.tanh(z2)\n",
    "    a2 = softmax(z2)\n",
    "    \n",
    "    #Third linear step\n",
    "    z3 = a2.dot(W3) + b3\n",
    "    \n",
    "    #For the Third linear activation function we use the softmax function, \n",
    "        #either the sigmoid of softmax should be used for the last layer\n",
    "    a3 = softmax(z3)\n",
    "    \n",
    "    #Store all results in these values\n",
    "    cache = {'a0':a0,'z1':z1,'a1':a1,'z2':z2,'a2':a2,'a3':a3,'z3':z3}\n",
    "    return cache\n",
    "\n",
    "# This is the BACKWARD PROPAGATION function\n",
    "def backward_prop(model,cache,y):\n",
    "\n",
    "    # Load parameters from model\n",
    "    W1, b1, W2, b2, W3, b3 = model['W1'], model['b1'], model['W2'], model['b2'],model['W3'],model['b3']\n",
    "    \n",
    "    # Load forward propagation results\n",
    "    a0, a1, a2, a3 = cache['a0'],cache['a1'],cache['a2'],cache['a3']\n",
    "    \n",
    "    # Get number of samples\n",
    "    m = y.shape[0]\n",
    "    \n",
    "    # Calculate loss derivative with respect to output\n",
    "    dz3 = loss_derivative(y_true=y,y_hat=a3)\n",
    "\n",
    "    # Calculate loss derivative with respect to second layer weights\n",
    "    dW3 = 1/m*(a2.T).dot(dz3)\n",
    "    \n",
    "    # Calculate loss derivative with respect to second layer bias\n",
    "    db3 = 1/m*np.sum(dz3, axis=0)\n",
    "    \n",
    "    # Calculate loss derivative with respect to first layer\n",
    "    dz2 = np.multiply(dz3.dot(W3.T) ,tanh_derivative(a2))\n",
    "#     dz2 = np.multiply(dz3.dot(W3.T) ,softmax_grad(a2))\n",
    "    \n",
    "    # Calculate loss derivative with respect to first layer weights\n",
    "    dW2 = 1/m*np.dot(a1.T, dz2)\n",
    "    \n",
    "    # Calculate loss derivative with respect to first layer bias\n",
    "    db2 = 1/m*np.sum(dz2, axis=0)\n",
    "    \n",
    "    dz1 = np.multiply(dz2.dot(W2.T),tanh_derivative(a1))\n",
    "#     dz1 = np.multiply(dz2.dot(W2.T) ,softmax_grad(a1))\n",
    "    \n",
    "    dW1 = 1/m*np.dot(a0.T,dz1)\n",
    "    \n",
    "    db1 = 1/m*np.sum(dz1,axis=0)\n",
    "    \n",
    "    # Store gradients\n",
    "    grads = {'dW3':dW3, 'db3':db3, 'dW2':dW2,'db2':db2,'dW1':dW1,'db1':db1}\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we forward propagate through our NN, we backward propagate our error gradient to update our weight parameters. We know our error, and want to minimize it as much as possible.\n",
    "\n",
    "We do this by taking the derivative of the error function, with respect to the weights (W) of our NN, using gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(model,grads,learning_rate):\n",
    "    # Load parameters\n",
    "    W1, b1, W2, b2,b3,W3 = model['W1'], model['b1'], model['W2'], model['b2'],model['b3'],model[\"W3\"]\n",
    "    \n",
    "    # Update parameters\n",
    "    W1 -= learning_rate * grads['dW1']\n",
    "    b1 -= learning_rate * grads['db1']\n",
    "    W2 -= learning_rate * grads['dW2']\n",
    "    b2 -= learning_rate * grads['db2']\n",
    "    W3 -= learning_rate * grads['dW3']\n",
    "    b3 -= learning_rate * grads['db3']\n",
    "    \n",
    "    # Store and return parameters\n",
    "    model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2, 'W3':W3,'b3':b3}\n",
    "    return model\n",
    "def predict(model, x):\n",
    "    # Do forward pass\n",
    "    c = forward_prop(model,x)\n",
    "    #get y_hat\n",
    "    y_hat = np.argmax(c['a3'], axis=1)\n",
    "    return y_hat\n",
    "def calc_accuracy(model,x,y):\n",
    "    # Get total number of examples\n",
    "    m = y.shape[0]\n",
    "    # Do a prediction with the model\n",
    "    pred = predict(model,x)\n",
    "    # Ensure prediction and truth vector y have the same shape\n",
    "    pred = pred.reshape(y.shape)\n",
    "    # Calculate the number of wrong examples\n",
    "    error = np.sum(np.abs(pred-y))\n",
    "    # Calculate accuracy\n",
    "    return (m - error)/m * 100\n",
    "losses = []\n",
    "def train(model,X_,y_,learning_rate, epochs=2000, print_loss=False):\n",
    "    # Gradient descent. Loop over epochs\n",
    "    for i in range(0, epochs):\n",
    "\n",
    "        # Forward propagation\n",
    "        cache = forward_prop(model,X_)\n",
    "        #a1, probs = cache['a1'],cache['a2']\n",
    "        # Backpropagation\n",
    "        \n",
    "        grads = backward_prop(model,cache,y_)\n",
    "        # Gradient descent parameter update\n",
    "        # Assign new parameters to the model\n",
    "        model = update_parameters(model=model,grads=grads,learning_rate=learning_rate)\n",
    "    \n",
    "        # Pring loss & accuracy every 100 iterations\n",
    "        if print_loss and i % 100 == 0:\n",
    "            a3 = cache['a3']\n",
    "            print('Loss after iteration',i,':',softmax_loss(y_,a3))\n",
    "            y_hat = predict(model,X_)\n",
    "            y_true = y_.argmax(axis=1)\n",
    "            print('Accuracy after iteration',i,':',accuracy_score(y_pred=y_hat,y_true=y_true)*100,'%')\n",
    "            losses.append(accuracy_score(y_pred=y_hat,y_true=y_true)*100)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (178,13) and (8,5) not aligned: 13 (dim 1) != 8 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m----------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-1a9e2390cc5c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# This is what we return at the end\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minit_wts_bias\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlyr1\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mn1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlyr2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mn3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mprint_loss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-c3265a994aa2>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, X_, y_, learning_rate, epochs, print_loss)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[1;31m# Forward propagation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m         \u001b[0mcache\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mforward_prop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m         \u001b[1;31m#a1, probs = cache['a1'],cache['a2']\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[1;31m# Backpropagation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-d4510e5a4373>\u001b[0m in \u001b[0;36mforward_prop\u001b[1;34m(model, a0)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;31m# Second linear step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m     \u001b[0mz2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mb2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;31m# Second activation function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: shapes (178,13) and (8,5) not aligned: 13 (dim 1) != 8 (dim 0)"
     ]
    }
   ],
   "source": [
    "# np.random.seed(0)\n",
    "# This is what we return at the end\n",
    "model = init_wts_bias(inputs=n0, lyr1= n1, lyr2=n2, outputs= n3)\n",
    "model = train(model,X,y,learning_rate=lr,epochs=epoch,print_loss=True)\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by feeding data into the neural network and perform several matrix operations on this input data, layer by layer. For each of our three layers, we take the dot product of the input by the weights and add a bias. Next, we pass this output through an activation function of choice.\n",
    "\n",
    "The output of this activation function is then used as an input for the following layer to follow the same procedure. This process is iterated three times since we have three layers. Our final output is y-hat, which is the prediction on which wine belongs to which cultivar. This is the end of the forward propagation process.\n",
    "\n",
    "We then calculate the difference between our prediction (y-hat) and the expected output (y) and use this error value during backpropagation.\n",
    "\n",
    "During backpropagation, we take our error — the difference between our prediction y-hat and y — and we mathematically push it back through the NN in the other direction. We are learning from our mistakes.\n",
    "\n",
    "By taking the derivative of the functions we used during the first process, we try to discover what value we should give the weights in order to achieve the best possible prediction. Essentially we want to know what the relationship is between the value of our weight and the error that we get out as the result.\n",
    "\n",
    "And after many epochs or iterations, the NN has learned to give us more accurate predictions by adapting its parameters to our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

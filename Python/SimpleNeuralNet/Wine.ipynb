{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A From Scratch 3 Layer Neural Network (NN) Example\n",
    "\n",
    "This is a simple example of building a neural net from scratch using python without using a deep learning library like tensorflow. I will try to spare folks the math while attempting to demystify NN in plain english as much as possible. The solution assumes that you have some idea of what a neural net is. If you do not what it is or would like a refresher, check out the first of the 4 awesome videos on youtube: https://www.youtube.com/watch?v=aircAruvnKk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "A farmer in Italy was having a problem with his labelling machine: it mixed up the labels of three wine cultivars. Now he has 178 bottles left, and nobody knows which cultivar made them! To help this poor man, we will build a classifier that recognizes the wine based on 13 attributes of the wine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.freecodecamp.org/building-a-3-layer-neural-network-from-scratch-99239c4af5d3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before approaching our solution from a neural net's perspective, let us read & get some basic understanding of the data first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing all the required libraries\n",
    "\n",
    "# modules for data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# visualization modules\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# machine learning library and modules\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining path to the data and the data source itself\n",
    "rel_data_path=\"../../data/\"\n",
    "filename=\"Wine.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Malic acid</th>\n",
       "      <th>Ash</th>\n",
       "      <th>Alcalinity of ash</th>\n",
       "      <th>Magnesium</th>\n",
       "      <th>Total phenols</th>\n",
       "      <th>Flavanoids</th>\n",
       "      <th>Nonflavanoid phenols</th>\n",
       "      <th>Proanthocyanins</th>\n",
       "      <th>Color intensity</th>\n",
       "      <th>Hue</th>\n",
       "      <th>OD280/OD315 of diluted wines</th>\n",
       "      <th>Proline</th>\n",
       "      <th>Cultivar 1</th>\n",
       "      <th>Cultivar 2</th>\n",
       "      <th>Cultivar 3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.518613</td>\n",
       "      <td>-0.562250</td>\n",
       "      <td>0.232053</td>\n",
       "      <td>-1.169593</td>\n",
       "      <td>1.913905</td>\n",
       "      <td>0.808997</td>\n",
       "      <td>1.034819</td>\n",
       "      <td>-0.659563</td>\n",
       "      <td>1.224884</td>\n",
       "      <td>0.251717</td>\n",
       "      <td>0.362177</td>\n",
       "      <td>1.847920</td>\n",
       "      <td>1.013009</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.246290</td>\n",
       "      <td>-0.499413</td>\n",
       "      <td>-0.827996</td>\n",
       "      <td>-2.490847</td>\n",
       "      <td>0.018145</td>\n",
       "      <td>0.568648</td>\n",
       "      <td>0.733629</td>\n",
       "      <td>-0.820719</td>\n",
       "      <td>-0.544721</td>\n",
       "      <td>-0.293321</td>\n",
       "      <td>0.406051</td>\n",
       "      <td>1.113449</td>\n",
       "      <td>0.965242</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.196879</td>\n",
       "      <td>0.021231</td>\n",
       "      <td>1.109334</td>\n",
       "      <td>-0.268738</td>\n",
       "      <td>0.088358</td>\n",
       "      <td>0.808997</td>\n",
       "      <td>1.215533</td>\n",
       "      <td>-0.498407</td>\n",
       "      <td>2.135968</td>\n",
       "      <td>0.269020</td>\n",
       "      <td>0.318304</td>\n",
       "      <td>0.788587</td>\n",
       "      <td>1.395148</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.691550</td>\n",
       "      <td>-0.346811</td>\n",
       "      <td>0.487926</td>\n",
       "      <td>-0.809251</td>\n",
       "      <td>0.930918</td>\n",
       "      <td>2.491446</td>\n",
       "      <td>1.466525</td>\n",
       "      <td>-0.981875</td>\n",
       "      <td>1.032155</td>\n",
       "      <td>1.186068</td>\n",
       "      <td>-0.427544</td>\n",
       "      <td>1.184071</td>\n",
       "      <td>2.334574</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.295700</td>\n",
       "      <td>0.227694</td>\n",
       "      <td>1.840403</td>\n",
       "      <td>0.451946</td>\n",
       "      <td>1.281985</td>\n",
       "      <td>0.808997</td>\n",
       "      <td>0.663351</td>\n",
       "      <td>0.226796</td>\n",
       "      <td>0.401404</td>\n",
       "      <td>-0.319276</td>\n",
       "      <td>0.362177</td>\n",
       "      <td>0.449601</td>\n",
       "      <td>-0.037874</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Alcohol  Malic acid       Ash  Alcalinity of ash  Magnesium  \\\n",
       "0  1.518613   -0.562250  0.232053          -1.169593   1.913905   \n",
       "1  0.246290   -0.499413 -0.827996          -2.490847   0.018145   \n",
       "2  0.196879    0.021231  1.109334          -0.268738   0.088358   \n",
       "3  1.691550   -0.346811  0.487926          -0.809251   0.930918   \n",
       "4  0.295700    0.227694  1.840403           0.451946   1.281985   \n",
       "\n",
       "   Total phenols  Flavanoids  Nonflavanoid phenols  Proanthocyanins  \\\n",
       "0       0.808997    1.034819             -0.659563         1.224884   \n",
       "1       0.568648    0.733629             -0.820719        -0.544721   \n",
       "2       0.808997    1.215533             -0.498407         2.135968   \n",
       "3       2.491446    1.466525             -0.981875         1.032155   \n",
       "4       0.808997    0.663351              0.226796         0.401404   \n",
       "\n",
       "   Color intensity       Hue  OD280/OD315 of diluted wines   Proline  \\\n",
       "0         0.251717  0.362177                      1.847920  1.013009   \n",
       "1        -0.293321  0.406051                      1.113449  0.965242   \n",
       "2         0.269020  0.318304                      0.788587  1.395148   \n",
       "3         1.186068 -0.427544                      1.184071  2.334574   \n",
       "4        -0.319276  0.362177                      0.449601 -0.037874   \n",
       "\n",
       "   Cultivar 1  Cultivar 2  Cultivar 3  \n",
       "0           1           0           0  \n",
       "1           1           0           0  \n",
       "2           1           0           0  \n",
       "3           1           0           0  \n",
       "4           1           0           0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = rel_data_path+filename\n",
    "df = pd.read_csv(file_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Alcohol', 'Malic acid', 'Ash', 'Alcalinity of ash', 'Magnesium',\n",
       "       'Total phenols', 'Flavanoids', 'Nonflavanoid phenols',\n",
       "       'Proanthocyanins', 'Color intensity', 'Hue',\n",
       "       'OD280/OD315 of diluted wines', 'Proline', 'Cultivar 1', 'Cultivar 2',\n",
       "       'Cultivar 3'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the sample data and the column names, we know that the farmer is looking to determine the value for one of the cultivar and wants it to be as accurate as possible based on the other attributes. Our dataset gives us a value of the output that we can use to determine how good the prediction is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a NN\n",
    "\n",
    "#### Defining X & y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get labels\n",
    "y = df[['Cultivar 1', 'Cultivar 2', 'Cultivar 3']].values\n",
    "\n",
    "# Get inputs; \n",
    "X = df.drop(['Cultivar 1', 'Cultivar 2', 'Cultivar 3'], axis = 1)\n",
    "# print(X.shape, y.shape) # Print shapes just to check\n",
    "X = X.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identifying all needed data and values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `X` here consists of 178 rows with 13 columns or 13 _neurons_. This is the input layer of the NN. The `y` is the output and consists of 3 neurons (columns). The goal is to match each input row with one of the output columns using the 13 input columns.\n",
    "\n",
    "The output is considered to be one of the layers and the input typically is not. Hence, for a 3 layer NN, we still need two more layers. This is really an arbitrary number and I am using 3 layer NN as an example. Each of these layers will also have an arbitrary number of neurons which will be defined as we progress. \n",
    "\n",
    "Here is what we need:\n",
    "\n",
    "* An input layer, `X` (already defined) with `n0` neurons\n",
    "* 2 hidden layers `A1` & `A2` with arbitrary number of neurons `n1` & `n2` respectively\n",
    "* A predicted output layer, `y_hat` with `n3` neurons\n",
    "* A choice of activation function `σ1` & `σ2`  for each hidden layer\n",
    "* A set of weights `W1`, `W2` & `W3`, and biases `b1`, `b2`, `b3` between each layer\n",
    "* An `epoch` or number of iterations before a final output is determined\n",
    "* A learning rate `lr` for the model. The learning rate determines how rapidly can the parameters change. If the learning rate is very low, training takes longer. However, if the learning rate is too high, there is a chance to miss the minimum. Choosing a learning rate may involve some trial and error and is beyond the scope of this notebook.\n",
    "\n",
    "A NN process will follow the following steps and use the variables defined above to generate the desired output.\n",
    "1. Take input\n",
    "2. Mutiply 1 by a set of weights and add the biases\n",
    "3. Apply an activation function to 2\n",
    "4. Repeat steps 2 & 3 for each layer\n",
    "5. Return an output\n",
    "6. Calculate error\n",
    "7. Alter the weights based on the error\n",
    "8. Go back to step 2 and repeat for a predefined number of times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining all needed data and values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.02 # learning rate\n",
    "epoch = 4500 # again an arbitrary number for number of iterations\n",
    "n0 = 13 # input layer \n",
    "n1 = 8 # hidden layer 1\n",
    "n2 = 5 # hidden layer 2\n",
    "n3 = 3 # output layer 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we do not have any values of either the weights `Wi` or the bias `bi`, we will initialize these values using a random number. The weights will be between 0 & 1 and we will start with the biases of 0. Wrapping all these weights and biases in a function will allow us to modularize the code and make it easy to trouble shoot later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0) # to create a reproducible example\n",
    "\n",
    "def init_wts_bias(inputs,lyr1,lyr2,outputs):\n",
    "    \n",
    "    # First layer weights & bias\n",
    "#     W1 = 2 * np.random.randn(inputs, lyr1) - 1\n",
    "    W1 = np.random.random((inputs, lyr1))\n",
    "    b1 = np.zeros((1, lyr1))\n",
    "    \n",
    "    print(W1)\n",
    "    \n",
    "    # Second layer weights & bias\n",
    "#     W2 = 2 * np.random.randn(lyr1,lyr2) - 1\n",
    "    W2 = np.random.random((lyr1,lyr2))\n",
    "    b2 = np.zeros((1, lyr2))\n",
    "#     print(W2)\n",
    "    \n",
    "    # Third layer weights & bias\n",
    "#     W3 = 2 * np.random.rand(lyr2, outputs) - 1\n",
    "    W3 = np.random.random((lyr2, outputs))\n",
    "    b3 = np.zeros((1,outputs))\n",
    "#     print(W3)\n",
    "    \n",
    "    # Package and return model\n",
    "    model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2,'W3':W3,'b3':b3}\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5488135  0.71518937 0.60276338 0.54488318 0.4236548  0.64589411\n",
      "  0.43758721 0.891773  ]\n",
      " [0.96366276 0.38344152 0.79172504 0.52889492 0.56804456 0.92559664\n",
      "  0.07103606 0.0871293 ]\n",
      " [0.0202184  0.83261985 0.77815675 0.87001215 0.97861834 0.79915856\n",
      "  0.46147936 0.78052918]\n",
      " [0.11827443 0.63992102 0.14335329 0.94466892 0.52184832 0.41466194\n",
      "  0.26455561 0.77423369]\n",
      " [0.45615033 0.56843395 0.0187898  0.6176355  0.61209572 0.616934\n",
      "  0.94374808 0.6818203 ]\n",
      " [0.3595079  0.43703195 0.6976312  0.06022547 0.66676672 0.67063787\n",
      "  0.21038256 0.1289263 ]\n",
      " [0.31542835 0.36371077 0.57019677 0.43860151 0.98837384 0.10204481\n",
      "  0.20887676 0.16130952]\n",
      " [0.65310833 0.2532916  0.46631077 0.24442559 0.15896958 0.11037514\n",
      "  0.65632959 0.13818295]\n",
      " [0.19658236 0.36872517 0.82099323 0.09710128 0.83794491 0.09609841\n",
      "  0.97645947 0.4686512 ]\n",
      " [0.97676109 0.60484552 0.73926358 0.03918779 0.28280696 0.12019656\n",
      "  0.2961402  0.11872772]\n",
      " [0.31798318 0.41426299 0.0641475  0.69247212 0.56660145 0.26538949\n",
      "  0.52324805 0.09394051]\n",
      " [0.5759465  0.9292962  0.31856895 0.66741038 0.13179786 0.7163272\n",
      "  0.28940609 0.18319136]\n",
      " [0.58651293 0.02010755 0.82894003 0.00469548 0.67781654 0.27000797\n",
      "  0.73519402 0.96218855]]\n"
     ]
    }
   ],
   "source": [
    "temp = init_wts_bias(13,8,5,3)\n",
    "# temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we define all our functions\n",
    "\n",
    "# Since this is a multi-class classification problem (we have 3 output labels), \n",
    "    # we will use the softmax function for the output layer — A3 — \n",
    "    # because this will compute the probabilities for the classes by spitting out a value between 0 and 1.\n",
    "def softmax(z):\n",
    "    #Calculate exponent term firsta\n",
    "    exp_scores = np.exp(z)\n",
    "    return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "\n",
    "def softmax_loss(y,y_hat):\n",
    "    # Clipping value\n",
    "    minval = 0.000000000001\n",
    "    # Number of samples\n",
    "    m = y.shape[0]\n",
    "    # Loss formula, note that np.sum sums up the entire matrix and therefore does the job of two sums from the formula\n",
    "    loss = -1/m * np.sum(y * np.log(y_hat.clip(min=minval)))\n",
    "    return loss\n",
    "\n",
    "def loss_derivative(y,y_hat):\n",
    "    return (y_hat-y)\n",
    "\n",
    "\n",
    "# hyperbolic tangent (tanh) activation function. Can be otherfunctions like sigmoid.\n",
    "# For this model, we chose to use the tanh activation function for our two hidden layers — A1 and A2 — \n",
    "    # which gives us an output value between 0 and -1.\n",
    "def tanh_derivative(x): \n",
    "    return (1 - np.power(x, 2))\n",
    "\n",
    "# This is the forward propagation function\n",
    "def forward_prop(model,a0):\n",
    "    \n",
    "    #Start Forward Propagation\n",
    "    \n",
    "    # Load parameters from model\n",
    "    W1, b1, W2, b2, W3, b3 = model['W1'], model['b1'], model['W2'], model['b2'], model['W3'],model['b3']\n",
    "    \n",
    "    # Do the first Linear step \n",
    "    # Z1 is the input layer x times the dot product of the weights + our bias b\n",
    "    z1 = a0.dot(W1) + b1\n",
    "    \n",
    "    # Put it through the first activation function. Also called as transfer function.\n",
    "    a1 = np.tanh(z1)\n",
    "    \n",
    "    # Second linear step\n",
    "    z2 = a1.dot(W2) + b2\n",
    "    \n",
    "    # Second activation function\n",
    "    a2 = np.tanh(z2)\n",
    "    \n",
    "    #Third linear step\n",
    "    z3 = a2.dot(W3) + b3\n",
    "    \n",
    "    #For the Third linear activation function we use the softmax function, \n",
    "        #either the sigmoid of softmax should be used for the last layer\n",
    "    a3 = softmax(z3)\n",
    "    \n",
    "    #Store all results in these values\n",
    "    cache = {'a0':a0,'z1':z1,'a1':a1,'z2':z2,'a2':a2,'a3':a3,'z3':z3}\n",
    "    return cache\n",
    "\n",
    "# This is the BACKWARD PROPAGATION function\n",
    "def backward_prop(model,cache,y):\n",
    "\n",
    "    # Load parameters from model\n",
    "    W1, b1, W2, b2, W3, b3 = model['W1'], model['b1'], model['W2'], model['b2'],model['W3'],model['b3']\n",
    "    \n",
    "    # Load forward propagation results\n",
    "    a0,a1, a2,a3 = cache['a0'],cache['a1'],cache['a2'],cache['a3']\n",
    "    \n",
    "    # Get number of samples\n",
    "    m = y.shape[0]\n",
    "    \n",
    "    # Calculate loss derivative with respect to output\n",
    "    dz3 = loss_derivative(y=y,y_hat=a3)\n",
    "\n",
    "    # Calculate loss derivative with respect to second layer weights\n",
    "    dW3 = 1/m*(a2.T).dot(dz3) #dW2 = 1/m*(a1.T).dot(dz2) \n",
    "    \n",
    "    # Calculate loss derivative with respect to second layer bias\n",
    "    db3 = 1/m*np.sum(dz3, axis=0)\n",
    "    \n",
    "    # Calculate loss derivative with respect to first layer\n",
    "    dz2 = np.multiply(dz3.dot(W3.T) ,tanh_derivative(a2))\n",
    "    \n",
    "    # Calculate loss derivative with respect to first layer weights\n",
    "    dW2 = 1/m*np.dot(a1.T, dz2)\n",
    "    \n",
    "    # Calculate loss derivative with respect to first layer bias\n",
    "    db2 = 1/m*np.sum(dz2, axis=0)\n",
    "    \n",
    "    dz1 = np.multiply(dz2.dot(W2.T),tanh_derivative(a1))\n",
    "    \n",
    "    dW1 = 1/m*np.dot(a0.T,dz1)\n",
    "    \n",
    "    db1 = 1/m*np.sum(dz1,axis=0)\n",
    "    \n",
    "    # Store gradients\n",
    "    grads = {'dW3':dW3, 'db3':db3, 'dW2':dW2,'db2':db2,'dW1':dW1,'db1':db1}\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we forward propagate through our NN, we backward propagate our error gradient to update our weight parameters. We know our error, and want to minimize it as much as possible.\n",
    "\n",
    "We do this by taking the derivative of the error function, with respect to the weights (W) of our NN, using gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(model,grads,learning_rate):\n",
    "    # Load parameters\n",
    "    W1, b1, W2, b2,b3,W3 = model['W1'], model['b1'], model['W2'], model['b2'],model['b3'],model[\"W3\"]\n",
    "    \n",
    "    # Update parameters\n",
    "    W1 -= learning_rate * grads['dW1']\n",
    "    b1 -= learning_rate * grads['db1']\n",
    "    W2 -= learning_rate * grads['dW2']\n",
    "    b2 -= learning_rate * grads['db2']\n",
    "    W3 -= learning_rate * grads['dW3']\n",
    "    b3 -= learning_rate * grads['db3']\n",
    "    \n",
    "    # Store and return parameters\n",
    "    model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2, 'W3':W3,'b3':b3}\n",
    "    return model\n",
    "def predict(model, x):\n",
    "    # Do forward pass\n",
    "    c = forward_prop(model,x)\n",
    "    #get y_hat\n",
    "    y_hat = np.argmax(c['a3'], axis=1)\n",
    "    return y_hat\n",
    "def calc_accuracy(model,x,y):\n",
    "    # Get total number of examples\n",
    "    m = y.shape[0]\n",
    "    # Do a prediction with the model\n",
    "    pred = predict(model,x)\n",
    "    # Ensure prediction and truth vector y have the same shape\n",
    "    pred = pred.reshape(y.shape)\n",
    "    # Calculate the number of wrong examples\n",
    "    error = np.sum(np.abs(pred-y))\n",
    "    # Calculate accuracy\n",
    "    return (m - error)/m * 100\n",
    "losses = []\n",
    "def train(model,X_,y_,learning_rate, epochs=20000, print_loss=False):\n",
    "    # Gradient descent. Loop over epochs\n",
    "    for i in range(0, epochs):\n",
    "\n",
    "        # Forward propagation\n",
    "        cache = forward_prop(model,X_)\n",
    "        #a1, probs = cache['a1'],cache['a2']\n",
    "        # Backpropagation\n",
    "        \n",
    "        grads = backward_prop(model,cache,y_)\n",
    "        # Gradient descent parameter update\n",
    "        # Assign new parameters to the model\n",
    "        model = update_parameters(model=model,grads=grads,learning_rate=learning_rate)\n",
    "    \n",
    "        # Pring loss & accuracy every 100 iterations\n",
    "        if print_loss and i % 100 == 0:\n",
    "            a3 = cache['a3']\n",
    "            print('Loss after iteration',i,':',softmax_loss(y_,a3))\n",
    "            y_hat = predict(model,X_)\n",
    "            y_true = y_.argmax(axis=1)\n",
    "            print('Accuracy after iteration',i,':',accuracy_score(y_pred=y_hat,y_true=y_true)*100,'%')\n",
    "            losses.append(accuracy_score(y_pred=y_hat,y_true=y_true)*100)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.069167   0.69742877 0.45354268 0.7220556  0.86638233 0.97552151\n",
      "  0.85580334 0.01171408]\n",
      " [0.35997806 0.72999056 0.17162968 0.52103661 0.05433799 0.19999652\n",
      "  0.01852179 0.7936977 ]\n",
      " [0.22392469 0.34535168 0.92808129 0.7044144  0.03183893 0.16469416\n",
      "  0.6214784  0.57722859]\n",
      " [0.23789282 0.934214   0.61396596 0.5356328  0.58990998 0.73012203\n",
      "  0.311945   0.39822106]\n",
      " [0.20984375 0.18619301 0.94437239 0.7395508  0.49045881 0.22741463\n",
      "  0.25435648 0.05802916]\n",
      " [0.43441663 0.31179588 0.69634349 0.37775184 0.17960368 0.02467873\n",
      "  0.06724963 0.67939277]\n",
      " [0.45369684 0.53657921 0.89667129 0.99033895 0.21689698 0.6630782\n",
      "  0.26332238 0.020651  ]\n",
      " [0.75837865 0.32001715 0.38346389 0.58831711 0.83104846 0.62898184\n",
      "  0.87265066 0.27354203]\n",
      " [0.79804683 0.18563594 0.95279166 0.68748828 0.21550768 0.94737059\n",
      "  0.73085581 0.25394164]\n",
      " [0.21331198 0.51820071 0.02566272 0.20747008 0.42468547 0.37416998\n",
      "  0.46357542 0.27762871]\n",
      " [0.58678435 0.86385561 0.11753186 0.51737911 0.13206811 0.71685968\n",
      "  0.3960597  0.56542131]\n",
      " [0.18327984 0.14484776 0.48805628 0.35561274 0.94043195 0.76532525\n",
      "  0.74866362 0.90371974]\n",
      " [0.08342244 0.55219247 0.58447607 0.96193638 0.29214753 0.24082878\n",
      "  0.10029394 0.01642963]]\n",
      "Loss after iteration 0 : 1.7829841689698005\n",
      "Accuracy after iteration 0 : 8.98876404494382 %\n",
      "Loss after iteration 100 : 0.7820387775891312\n",
      "Accuracy after iteration 100 : 65.1685393258427 %\n",
      "Loss after iteration 200 : 0.7273348370527413\n",
      "Accuracy after iteration 200 : 68.53932584269663 %\n",
      "Loss after iteration 300 : 0.6761517017363996\n",
      "Accuracy after iteration 300 : 70.2247191011236 %\n",
      "Loss after iteration 400 : 0.6257107638846103\n",
      "Accuracy after iteration 400 : 74.15730337078652 %\n",
      "Loss after iteration 500 : 0.5816638445328333\n",
      "Accuracy after iteration 500 : 76.96629213483146 %\n",
      "Loss after iteration 600 : 0.5400362818957088\n",
      "Accuracy after iteration 600 : 78.65168539325843 %\n",
      "Loss after iteration 700 : 0.4864264836482397\n",
      "Accuracy after iteration 700 : 79.7752808988764 %\n",
      "Loss after iteration 800 : 0.4169928345868556\n",
      "Accuracy after iteration 800 : 82.58426966292134 %\n",
      "Loss after iteration 900 : 0.36147859441465446\n",
      "Accuracy after iteration 900 : 85.39325842696628 %\n",
      "Loss after iteration 1000 : 0.3197138293021985\n",
      "Accuracy after iteration 1000 : 87.64044943820225 %\n",
      "Loss after iteration 1100 : 0.2857567731916429\n",
      "Accuracy after iteration 1100 : 92.69662921348315 %\n",
      "Loss after iteration 1200 : 0.2477335551754804\n",
      "Accuracy after iteration 1200 : 92.69662921348315 %\n",
      "Loss after iteration 1300 : 0.20731530672252677\n",
      "Accuracy after iteration 1300 : 93.25842696629213 %\n",
      "Loss after iteration 1400 : 0.1728084090441921\n",
      "Accuracy after iteration 1400 : 94.3820224719101 %\n",
      "Loss after iteration 1500 : 0.1425022372567273\n",
      "Accuracy after iteration 1500 : 96.06741573033707 %\n",
      "Loss after iteration 1600 : 0.11321419799023558\n",
      "Accuracy after iteration 1600 : 97.75280898876404 %\n",
      "Loss after iteration 1700 : 0.09622414837670126\n",
      "Accuracy after iteration 1700 : 98.87640449438202 %\n",
      "Loss after iteration 1800 : 0.08554029351090928\n",
      "Accuracy after iteration 1800 : 98.87640449438202 %\n",
      "Loss after iteration 1900 : 0.07757984115204222\n",
      "Accuracy after iteration 1900 : 98.87640449438202 %\n",
      "Loss after iteration 2000 : 0.07133427443174781\n",
      "Accuracy after iteration 2000 : 98.87640449438202 %\n",
      "Loss after iteration 2100 : 0.06621785723139109\n",
      "Accuracy after iteration 2100 : 98.87640449438202 %\n",
      "Loss after iteration 2200 : 0.061739803633160506\n",
      "Accuracy after iteration 2200 : 98.87640449438202 %\n",
      "Loss after iteration 2300 : 0.05720396365901882\n",
      "Accuracy after iteration 2300 : 99.43820224719101 %\n",
      "Loss after iteration 2400 : 0.05112229836180354\n",
      "Accuracy after iteration 2400 : 99.43820224719101 %\n",
      "Loss after iteration 2500 : 0.04457670340659987\n",
      "Accuracy after iteration 2500 : 99.43820224719101 %\n",
      "Loss after iteration 2600 : 0.039844402510058205\n",
      "Accuracy after iteration 2600 : 99.43820224719101 %\n",
      "Loss after iteration 2700 : 0.03614138656421798\n",
      "Accuracy after iteration 2700 : 99.43820224719101 %\n",
      "Loss after iteration 2800 : 0.0330203538395254\n",
      "Accuracy after iteration 2800 : 99.43820224719101 %\n",
      "Loss after iteration 2900 : 0.030316778973671243\n",
      "Accuracy after iteration 2900 : 99.43820224719101 %\n",
      "Loss after iteration 3000 : 0.02794750228700033\n",
      "Accuracy after iteration 3000 : 99.43820224719101 %\n",
      "Loss after iteration 3100 : 0.02586125307104484\n",
      "Accuracy after iteration 3100 : 99.43820224719101 %\n",
      "Loss after iteration 3200 : 0.024021451336900555\n",
      "Accuracy after iteration 3200 : 99.43820224719101 %\n",
      "Loss after iteration 3300 : 0.02239788969709807\n",
      "Accuracy after iteration 3300 : 99.43820224719101 %\n",
      "Loss after iteration 3400 : 0.020963311387750923\n",
      "Accuracy after iteration 3400 : 100.0 %\n",
      "Loss after iteration 3500 : 0.01969269841627027\n",
      "Accuracy after iteration 3500 : 100.0 %\n",
      "Loss after iteration 3600 : 0.018563487957668552\n",
      "Accuracy after iteration 3600 : 100.0 %\n",
      "Loss after iteration 3700 : 0.017555815750696886\n",
      "Accuracy after iteration 3700 : 100.0 %\n",
      "Loss after iteration 3800 : 0.01665254192595367\n",
      "Accuracy after iteration 3800 : 100.0 %\n",
      "Loss after iteration 3900 : 0.01583908318279356\n",
      "Accuracy after iteration 3900 : 100.0 %\n",
      "Loss after iteration 4000 : 0.015103134630846607\n",
      "Accuracy after iteration 4000 : 100.0 %\n",
      "Loss after iteration 4100 : 0.014434353117114428\n",
      "Accuracy after iteration 4100 : 100.0 %\n",
      "Loss after iteration 4200 : 0.01382404798767579\n",
      "Accuracy after iteration 4200 : 100.0 %\n",
      "Loss after iteration 4300 : 0.013264902840153048\n",
      "Accuracy after iteration 4300 : 100.0 %\n",
      "Loss after iteration 4400 : 0.012750736810899775\n",
      "Accuracy after iteration 4400 : 100.0 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x181584fcef0>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# np.random.seed(0)\n",
    "# This is what we return at the end\n",
    "model = init_wts_bias(inputs=n0, lyr1= n1, lyr2=n2, outputs= n3)\n",
    "model = train(model,X,y,learning_rate=lr,epochs=epoch,print_loss=True)\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by feeding data into the neural network and perform several matrix operations on this input data, layer by layer. For each of our three layers, we take the dot product of the input by the weights and add a bias. Next, we pass this output through an activation function of choice.\n",
    "\n",
    "The output of this activation function is then used as an input for the following layer to follow the same procedure. This process is iterated three times since we have three layers. Our final output is y-hat, which is the prediction on which wine belongs to which cultivar. This is the end of the forward propagation process.\n",
    "\n",
    "We then calculate the difference between our prediction (y-hat) and the expected output (y) and use this error value during backpropagation.\n",
    "\n",
    "During backpropagation, we take our error — the difference between our prediction y-hat and y — and we mathematically push it back through the NN in the other direction. We are learning from our mistakes.\n",
    "\n",
    "By taking the derivative of the functions we used during the first process, we try to discover what value we should give the weights in order to achieve the best possible prediction. Essentially we want to know what the relationship is between the value of our weight and the error that we get out as the result.\n",
    "\n",
    "And after many epochs or iterations, the NN has learned to give us more accurate predictions by adapting its parameters to our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

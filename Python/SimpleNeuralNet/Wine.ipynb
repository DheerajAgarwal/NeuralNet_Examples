{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A From Scratch 3 Layer Neural Networks Example\n",
    "\n",
    "A farmer in Italy was having a problem with his labelling machine: it mixed up the labels of three wine cultivars. Now he has 178 bottles left, and nobody knows which cultivar made them! To help this poor man, we will build a classifier that recognizes the wine based on 13 attributes of the wine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.freecodecamp.org/building-a-3-layer-neural-network-from-scratch-99239c4af5d3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An input layer, x\n",
    "An arbitrary amount of hidden layers\n",
    "An output layer, ŷ\n",
    "A set of weights and biases between each layer, W and b\n",
    "A choice of activation function for each hidden layer, σ. \n",
    "\n",
    "\n",
    "In short:\n",
    "\n",
    "The input layer (x) consists of 178 neurons (rows of data).\n",
    "A1, the first layer, consists of 8 neurons.\n",
    "A2, the second layer, consists of 5 neurons.\n",
    "A3, the third and output layer, consists of 3 neurons (columns of data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Takes inputs as a matrix (2D array of numbers)\n",
    "\n",
    "Multiplies the input by a set weights (performs a dot product aka matrix multiplication)\n",
    "\n",
    "Applies an activation function\n",
    "\n",
    "Returns an output\n",
    "\n",
    "Error is calculated by taking the difference from the desired output from the data and the predicted output. This creates our gradient descent, which we can use to alter the weights\n",
    "\n",
    "The weights are then altered slightly according to the error.\n",
    "\n",
    "Before we can use our weights, we have to initialize them. Because we don’t have values to use for the weights yet, we use random values between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing all the libraries and dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#First we are importing all the libraries\n",
    "\n",
    "# Package imports\n",
    "# Matplotlib is a matlab like plotting library\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# SciKitLearn is a useful machine learning utilities library\n",
    "import sklearn\n",
    "# The sklearn dataset module helps generating datasets\n",
    "import sklearn.datasets\n",
    "import sklearn.linear_model\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>Malic acid</th>\n",
       "      <th>Ash</th>\n",
       "      <th>Alcalinity of ash</th>\n",
       "      <th>Magnesium</th>\n",
       "      <th>Total phenols</th>\n",
       "      <th>Flavanoids</th>\n",
       "      <th>Nonflavanoid phenols</th>\n",
       "      <th>Proanthocyanins</th>\n",
       "      <th>Color intensity</th>\n",
       "      <th>Hue</th>\n",
       "      <th>OD280/OD315 of diluted wines</th>\n",
       "      <th>Proline</th>\n",
       "      <th>Cultivar 1</th>\n",
       "      <th>Cultivar 2</th>\n",
       "      <th>Cultivar 3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.518613</td>\n",
       "      <td>-0.562250</td>\n",
       "      <td>0.232053</td>\n",
       "      <td>-1.169593</td>\n",
       "      <td>1.913905</td>\n",
       "      <td>0.808997</td>\n",
       "      <td>1.034819</td>\n",
       "      <td>-0.659563</td>\n",
       "      <td>1.224884</td>\n",
       "      <td>0.251717</td>\n",
       "      <td>0.362177</td>\n",
       "      <td>1.847920</td>\n",
       "      <td>1.013009</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.246290</td>\n",
       "      <td>-0.499413</td>\n",
       "      <td>-0.827996</td>\n",
       "      <td>-2.490847</td>\n",
       "      <td>0.018145</td>\n",
       "      <td>0.568648</td>\n",
       "      <td>0.733629</td>\n",
       "      <td>-0.820719</td>\n",
       "      <td>-0.544721</td>\n",
       "      <td>-0.293321</td>\n",
       "      <td>0.406051</td>\n",
       "      <td>1.113449</td>\n",
       "      <td>0.965242</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.196879</td>\n",
       "      <td>0.021231</td>\n",
       "      <td>1.109334</td>\n",
       "      <td>-0.268738</td>\n",
       "      <td>0.088358</td>\n",
       "      <td>0.808997</td>\n",
       "      <td>1.215533</td>\n",
       "      <td>-0.498407</td>\n",
       "      <td>2.135968</td>\n",
       "      <td>0.269020</td>\n",
       "      <td>0.318304</td>\n",
       "      <td>0.788587</td>\n",
       "      <td>1.395148</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.691550</td>\n",
       "      <td>-0.346811</td>\n",
       "      <td>0.487926</td>\n",
       "      <td>-0.809251</td>\n",
       "      <td>0.930918</td>\n",
       "      <td>2.491446</td>\n",
       "      <td>1.466525</td>\n",
       "      <td>-0.981875</td>\n",
       "      <td>1.032155</td>\n",
       "      <td>1.186068</td>\n",
       "      <td>-0.427544</td>\n",
       "      <td>1.184071</td>\n",
       "      <td>2.334574</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.295700</td>\n",
       "      <td>0.227694</td>\n",
       "      <td>1.840403</td>\n",
       "      <td>0.451946</td>\n",
       "      <td>1.281985</td>\n",
       "      <td>0.808997</td>\n",
       "      <td>0.663351</td>\n",
       "      <td>0.226796</td>\n",
       "      <td>0.401404</td>\n",
       "      <td>-0.319276</td>\n",
       "      <td>0.362177</td>\n",
       "      <td>0.449601</td>\n",
       "      <td>-0.037874</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Alcohol  Malic acid       Ash  Alcalinity of ash  Magnesium  \\\n",
       "0  1.518613   -0.562250  0.232053          -1.169593   1.913905   \n",
       "1  0.246290   -0.499413 -0.827996          -2.490847   0.018145   \n",
       "2  0.196879    0.021231  1.109334          -0.268738   0.088358   \n",
       "3  1.691550   -0.346811  0.487926          -0.809251   0.930918   \n",
       "4  0.295700    0.227694  1.840403           0.451946   1.281985   \n",
       "\n",
       "   Total phenols  Flavanoids  Nonflavanoid phenols  Proanthocyanins  \\\n",
       "0       0.808997    1.034819             -0.659563         1.224884   \n",
       "1       0.568648    0.733629             -0.820719        -0.544721   \n",
       "2       0.808997    1.215533             -0.498407         2.135968   \n",
       "3       2.491446    1.466525             -0.981875         1.032155   \n",
       "4       0.808997    0.663351              0.226796         0.401404   \n",
       "\n",
       "   Color intensity       Hue  OD280/OD315 of diluted wines   Proline  \\\n",
       "0         0.251717  0.362177                      1.847920  1.013009   \n",
       "1        -0.293321  0.406051                      1.113449  0.965242   \n",
       "2         0.269020  0.318304                      0.788587  1.395148   \n",
       "3         1.186068 -0.427544                      1.184071  2.334574   \n",
       "4        -0.319276  0.362177                      0.449601 -0.037874   \n",
       "\n",
       "   Cultivar 1  Cultivar 2  Cultivar 3  \n",
       "0           1           0           0  \n",
       "1           1           0           0  \n",
       "2           1           0           0  \n",
       "3           1           0           0  \n",
       "4           1           0           0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./data/Wine.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(178, 13) (178, 3)\n"
     ]
    }
   ],
   "source": [
    "# we define our x and y here.\n",
    "\n",
    "# Get labels\n",
    "y = df[['Cultivar 1', 'Cultivar 2', 'Cultivar 3']].values\n",
    "\n",
    "# Get inputs; \n",
    "X = df.drop(['Cultivar 1', 'Cultivar 2', 'Cultivar 3'], axis = 1)\n",
    "print(X.shape, y.shape) # Print shapes just to check\n",
    "X = X.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we define all our functions\n",
    "\n",
    "# Since this is a multi-class classification problem (we have 3 output labels), \n",
    "    # we will use the softmax function for the output layer — A3 — \n",
    "    # because this will compute the probabilities for the classes by spitting out a value between 0 and 1.\n",
    "def softmax(z):\n",
    "    #Calculate exponent term first\n",
    "    exp_scores = np.exp(z)\n",
    "    return exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "\n",
    "def softmax_loss(y,y_hat):\n",
    "    # Clipping value\n",
    "    minval = 0.000000000001\n",
    "    # Number of samples\n",
    "    m = y.shape[0]\n",
    "    # Loss formula, note that np.sum sums up the entire matrix and therefore does the job of two sums from the formula\n",
    "    loss = -1/m * np.sum(y * np.log(y_hat.clip(min=minval)))\n",
    "    return loss\n",
    "\n",
    "def loss_derivative(y,y_hat):\n",
    "    return (y_hat-y)\n",
    "\n",
    "\n",
    "# hyperbolic tangent (tanh) activation function. Can be otherfunctions like sigmoid.\n",
    "# For this model, we chose to use the tanh activation function for our two hidden layers — A1 and A2 — \n",
    "    # which gives us an output value between 0 and -1.\n",
    "def tanh_derivative(x): \n",
    "    return (1 - np.power(x, 2))\n",
    "\n",
    "# This is the forward propagation function\n",
    "def forward_prop(model,a0):\n",
    "    \n",
    "    #Start Forward Propagation\n",
    "    \n",
    "    # Load parameters from model\n",
    "    W1, b1, W2, b2, W3, b3 = model['W1'], model['b1'], model['W2'], model['b2'], model['W3'],model['b3']\n",
    "    \n",
    "    # Do the first Linear step \n",
    "    # Z1 is the input layer x times the dot product of the weights + our bias b\n",
    "    z1 = a0.dot(W1) + b1\n",
    "    \n",
    "    # Put it through the first activation function. Also called as transfer function.\n",
    "    a1 = np.tanh(z1)\n",
    "    \n",
    "    # Second linear step\n",
    "    z2 = a1.dot(W2) + b2\n",
    "    \n",
    "    # Second activation function\n",
    "    a2 = np.tanh(z2)\n",
    "    \n",
    "    #Third linear step\n",
    "    z3 = a2.dot(W3) + b3\n",
    "    \n",
    "    #For the Third linear activation function we use the softmax function, \n",
    "        #either the sigmoid of softmax should be used for the last layer\n",
    "    a3 = softmax(z3)\n",
    "    \n",
    "    #Store all results in these values\n",
    "    cache = {'a0':a0,'z1':z1,'a1':a1,'z2':z2,'a2':a2,'a3':a3,'z3':z3}\n",
    "    return cache\n",
    "\n",
    "# This is the BACKWARD PROPAGATION function\n",
    "def backward_prop(model,cache,y):\n",
    "\n",
    "    # Load parameters from model\n",
    "    W1, b1, W2, b2, W3, b3 = model['W1'], model['b1'], model['W2'], model['b2'],model['W3'],model['b3']\n",
    "    \n",
    "    # Load forward propagation results\n",
    "    a0,a1, a2,a3 = cache['a0'],cache['a1'],cache['a2'],cache['a3']\n",
    "    \n",
    "    # Get number of samples\n",
    "    m = y.shape[0]\n",
    "    \n",
    "    # Calculate loss derivative with respect to output\n",
    "    dz3 = loss_derivative(y=y,y_hat=a3)\n",
    "\n",
    "    # Calculate loss derivative with respect to second layer weights\n",
    "    dW3 = 1/m*(a2.T).dot(dz3) #dW2 = 1/m*(a1.T).dot(dz2) \n",
    "    \n",
    "    # Calculate loss derivative with respect to second layer bias\n",
    "    db3 = 1/m*np.sum(dz3, axis=0)\n",
    "    \n",
    "    # Calculate loss derivative with respect to first layer\n",
    "    dz2 = np.multiply(dz3.dot(W3.T) ,tanh_derivative(a2))\n",
    "    \n",
    "    # Calculate loss derivative with respect to first layer weights\n",
    "    dW2 = 1/m*np.dot(a1.T, dz2)\n",
    "    \n",
    "    # Calculate loss derivative with respect to first layer bias\n",
    "    db2 = 1/m*np.sum(dz2, axis=0)\n",
    "    \n",
    "    dz1 = np.multiply(dz2.dot(W2.T),tanh_derivative(a1))\n",
    "    \n",
    "    dW1 = 1/m*np.dot(a0.T,dz1)\n",
    "    \n",
    "    db1 = 1/m*np.sum(dz1,axis=0)\n",
    "    \n",
    "    # Store gradients\n",
    "    grads = {'dW3':dW3, 'db3':db3, 'dW2':dW2,'db2':db2,'dW1':dW1,'db1':db1}\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we forward propagate through our NN, we backward propagate our error gradient to update our weight parameters. We know our error, and want to minimize it as much as possible.\n",
    "\n",
    "We do this by taking the derivative of the error function, with respect to the weights (W) of our NN, using gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAINING PHASE\n",
    "def initialize_parameters(nn_input_dim,nn_hdim,nn_output_dim):\n",
    "    # First layer weights\n",
    "    W1 = 2 *np.random.randn(nn_input_dim, nn_hdim) - 1\n",
    "    \n",
    "    # First layer bias\n",
    "    b1 = np.zeros((1, nn_hdim))\n",
    "    \n",
    "    # Second layer weights\n",
    "    W2 = 2 * np.random.randn(nn_hdim, nn_hdim) - 1\n",
    "    \n",
    "    # Second layer bias\n",
    "    b2 = np.zeros((1, nn_hdim))\n",
    "    W3 = 2 * np.random.rand(nn_hdim, nn_output_dim) - 1\n",
    "    b3 = np.zeros((1,nn_output_dim))\n",
    "    \n",
    "    \n",
    "    # Package and return model\n",
    "    model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2,'W3':W3,'b3':b3}\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a neural network will have to undergo many epochs or iterations to give us an accurate prediction.\n",
    "\n",
    "you have to specify a “learning rate” for the model. The learning rate is the multiplier to update the parameters. It determines how rapidly they can change. If the learning rate is low, training will take longer. However, if the learning rate is too high, we might miss a minimum.\n",
    "a:= a - a* dL(w)/da\n",
    "\n",
    ":= means that this is a definition, not an equation or proven statement.\n",
    "a is the learning rate called alpha\n",
    "dL(w) is the derivative of the total loss with respect to our weight w\n",
    "da is the derivative of alpha\n",
    "\n",
    "We chose a learning rate of 0.07 after some experimenting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(model,grads,learning_rate):\n",
    "    # Load parameters\n",
    "    W1, b1, W2, b2,b3,W3 = model['W1'], model['b1'], model['W2'], model['b2'],model['b3'],model[\"W3\"]\n",
    "    \n",
    "    # Update parameters\n",
    "    W1 -= learning_rate * grads['dW1']\n",
    "    b1 -= learning_rate * grads['db1']\n",
    "    W2 -= learning_rate * grads['dW2']\n",
    "    b2 -= learning_rate * grads['db2']\n",
    "    W3 -= learning_rate * grads['dW3']\n",
    "    b3 -= learning_rate * grads['db3']\n",
    "    \n",
    "    # Store and return parameters\n",
    "    model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2, 'W3':W3,'b3':b3}\n",
    "    return model\n",
    "def predict(model, x):\n",
    "    # Do forward pass\n",
    "    c = forward_prop(model,x)\n",
    "    #get y_hat\n",
    "    y_hat = np.argmax(c['a3'], axis=1)\n",
    "    return y_hat\n",
    "def calc_accuracy(model,x,y):\n",
    "    # Get total number of examples\n",
    "    m = y.shape[0]\n",
    "    # Do a prediction with the model\n",
    "    pred = predict(model,x)\n",
    "    # Ensure prediction and truth vector y have the same shape\n",
    "    pred = pred.reshape(y.shape)\n",
    "    # Calculate the number of wrong examples\n",
    "    error = np.sum(np.abs(pred-y))\n",
    "    # Calculate accuracy\n",
    "    return (m - error)/m * 100\n",
    "losses = []\n",
    "def train(model,X_,y_,learning_rate, epochs=20000, print_loss=False):\n",
    "    # Gradient descent. Loop over epochs\n",
    "    for i in range(0, epochs):\n",
    "\n",
    "        # Forward propagation\n",
    "        cache = forward_prop(model,X_)\n",
    "        #a1, probs = cache['a1'],cache['a2']\n",
    "        # Backpropagation\n",
    "        \n",
    "        grads = backward_prop(model,cache,y_)\n",
    "        # Gradient descent parameter update\n",
    "        # Assign new parameters to the model\n",
    "        model = update_parameters(model=model,grads=grads,learning_rate=learning_rate)\n",
    "    \n",
    "        # Pring loss & accuracy every 100 iterations\n",
    "        if print_loss and i % 100 == 0:\n",
    "            a3 = cache['a3']\n",
    "            print('Loss after iteration',i,':',softmax_loss(y_,a3))\n",
    "            y_hat = predict(model,X_)\n",
    "            y_true = y_.argmax(axis=1)\n",
    "            print('Accuracy after iteration',i,':',accuracy_score(y_pred=y_hat,y_true=y_true)*100,'%')\n",
    "            losses.append(accuracy_score(y_pred=y_hat,y_true=y_true)*100)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after iteration 0 : 1.4504813381979211\n",
      "Accuracy after iteration 0 : 25.280898876404496 %\n",
      "Loss after iteration 100 : 0.5255210272876467\n",
      "Accuracy after iteration 100 : 80.89887640449437 %\n",
      "Loss after iteration 200 : 0.3964222056044325\n",
      "Accuracy after iteration 200 : 86.51685393258427 %\n",
      "Loss after iteration 300 : 0.34694059691857543\n",
      "Accuracy after iteration 300 : 87.64044943820225 %\n",
      "Loss after iteration 400 : 0.32062505328047924\n",
      "Accuracy after iteration 400 : 89.32584269662921 %\n",
      "Loss after iteration 500 : 0.30057660926569424\n",
      "Accuracy after iteration 500 : 89.32584269662921 %\n",
      "Loss after iteration 600 : 0.2775754239020053\n",
      "Accuracy after iteration 600 : 89.8876404494382 %\n",
      "Loss after iteration 700 : 0.25803463679942457\n",
      "Accuracy after iteration 700 : 90.4494382022472 %\n",
      "Loss after iteration 800 : 0.24518960914662333\n",
      "Accuracy after iteration 800 : 90.4494382022472 %\n",
      "Loss after iteration 900 : 0.23453472996993882\n",
      "Accuracy after iteration 900 : 90.4494382022472 %\n",
      "Loss after iteration 1000 : 0.22490185375676294\n",
      "Accuracy after iteration 1000 : 90.4494382022472 %\n",
      "Loss after iteration 1100 : 0.21649115993849932\n",
      "Accuracy after iteration 1100 : 92.13483146067416 %\n",
      "Loss after iteration 1200 : 0.20897333560828457\n",
      "Accuracy after iteration 1200 : 93.25842696629213 %\n",
      "Loss after iteration 1300 : 0.20186399701140648\n",
      "Accuracy after iteration 1300 : 93.25842696629213 %\n",
      "Loss after iteration 1400 : 0.18022550179247987\n",
      "Accuracy after iteration 1400 : 93.25842696629213 %\n",
      "Loss after iteration 1500 : 0.16690182286613942\n",
      "Accuracy after iteration 1500 : 93.82022471910112 %\n",
      "Loss after iteration 1600 : 0.15681341901851115\n",
      "Accuracy after iteration 1600 : 94.3820224719101 %\n",
      "Loss after iteration 1700 : 0.14891431157061527\n",
      "Accuracy after iteration 1700 : 94.9438202247191 %\n",
      "Loss after iteration 1800 : 0.14240653665707972\n",
      "Accuracy after iteration 1800 : 94.9438202247191 %\n",
      "Loss after iteration 1900 : 0.13578456173744088\n",
      "Accuracy after iteration 1900 : 95.50561797752809 %\n",
      "Loss after iteration 2000 : 0.13191466723315254\n",
      "Accuracy after iteration 2000 : 95.50561797752809 %\n",
      "Loss after iteration 2100 : 0.1290070757552617\n",
      "Accuracy after iteration 2100 : 95.50561797752809 %\n",
      "Loss after iteration 2200 : 0.1266137159634668\n",
      "Accuracy after iteration 2200 : 95.50561797752809 %\n",
      "Loss after iteration 2300 : 0.12454049447039091\n",
      "Accuracy after iteration 2300 : 95.50561797752809 %\n",
      "Loss after iteration 2400 : 0.12268163837704688\n",
      "Accuracy after iteration 2400 : 95.50561797752809 %\n",
      "Loss after iteration 2500 : 0.12096337880182337\n",
      "Accuracy after iteration 2500 : 95.50561797752809 %\n",
      "Loss after iteration 2600 : 0.1193160760444004\n",
      "Accuracy after iteration 2600 : 95.50561797752809 %\n",
      "Loss after iteration 2700 : 0.11764906434462095\n",
      "Accuracy after iteration 2700 : 96.06741573033707 %\n",
      "Loss after iteration 2800 : 0.11578680890335505\n",
      "Accuracy after iteration 2800 : 96.06741573033707 %\n",
      "Loss after iteration 2900 : 0.11312910792526915\n",
      "Accuracy after iteration 2900 : 96.06741573033707 %\n",
      "Loss after iteration 3000 : 0.10512216026989905\n",
      "Accuracy after iteration 3000 : 96.06741573033707 %\n",
      "Loss after iteration 3100 : 0.0852832501938459\n",
      "Accuracy after iteration 3100 : 97.19101123595506 %\n",
      "Loss after iteration 3200 : 0.0731093700259741\n",
      "Accuracy after iteration 3200 : 98.31460674157303 %\n",
      "Loss after iteration 3300 : 0.07094849409554865\n",
      "Accuracy after iteration 3300 : 98.31460674157303 %\n",
      "Loss after iteration 3400 : 0.0664704165188349\n",
      "Accuracy after iteration 3400 : 98.31460674157303 %\n",
      "Loss after iteration 3500 : 0.06349694100556133\n",
      "Accuracy after iteration 3500 : 98.31460674157303 %\n",
      "Loss after iteration 3600 : 0.06209091813348392\n",
      "Accuracy after iteration 3600 : 98.87640449438202 %\n",
      "Loss after iteration 3700 : 0.06098723303288365\n",
      "Accuracy after iteration 3700 : 98.87640449438202 %\n",
      "Loss after iteration 3800 : 0.05989614213136662\n",
      "Accuracy after iteration 3800 : 98.87640449438202 %\n",
      "Loss after iteration 3900 : 0.05868449560544278\n",
      "Accuracy after iteration 3900 : 98.87640449438202 %\n",
      "Loss after iteration 4000 : 0.05727076686257909\n",
      "Accuracy after iteration 4000 : 98.87640449438202 %\n",
      "Loss after iteration 4100 : 0.05544442941545926\n",
      "Accuracy after iteration 4100 : 98.87640449438202 %\n",
      "Loss after iteration 4200 : 0.05188248794580313\n",
      "Accuracy after iteration 4200 : 98.87640449438202 %\n",
      "Loss after iteration 4300 : 0.04372581671101876\n",
      "Accuracy after iteration 4300 : 99.43820224719101 %\n",
      "Loss after iteration 4400 : 0.038547256153145075\n",
      "Accuracy after iteration 4400 : 99.43820224719101 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x20793da7860>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "# This is what we return at the end\n",
    "model = initialize_parameters(nn_input_dim=13, nn_hdim= 5, nn_output_dim= 3)\n",
    "model = train(model,X,y,learning_rate=0.07,epochs=4500,print_loss=True)\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by feeding data into the neural network and perform several matrix operations on this input data, layer by layer. For each of our three layers, we take the dot product of the input by the weights and add a bias. Next, we pass this output through an activation function of choice.\n",
    "\n",
    "The output of this activation function is then used as an input for the following layer to follow the same procedure. This process is iterated three times since we have three layers. Our final output is y-hat, which is the prediction on which wine belongs to which cultivar. This is the end of the forward propagation process.\n",
    "\n",
    "We then calculate the difference between our prediction (y-hat) and the expected output (y) and use this error value during backpropagation.\n",
    "\n",
    "During backpropagation, we take our error — the difference between our prediction y-hat and y — and we mathematically push it back through the NN in the other direction. We are learning from our mistakes.\n",
    "\n",
    "By taking the derivative of the functions we used during the first process, we try to discover what value we should give the weights in order to achieve the best possible prediction. Essentially we want to know what the relationship is between the value of our weight and the error that we get out as the result.\n",
    "\n",
    "And after many epochs or iterations, the NN has learned to give us more accurate predictions by adapting its parameters to our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
